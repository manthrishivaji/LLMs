{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install rouge-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:46:05.940854Z","iopub.execute_input":"2024-12-12T15:46:05.941224Z","iopub.status.idle":"2024-12-12T15:46:17.174959Z","shell.execute_reply.started":"2024-12-12T15:46:05.941192Z","shell.execute_reply":"2024-12-12T15:46:17.173830Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=847f2cf4a8730312ca056b3742cfe118af89f4b71208594badcee5e24f28525d\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:20:25.914698Z","iopub.execute_input":"2024-12-12T15:20:25.915015Z","iopub.status.idle":"2024-12-12T15:20:28.806259Z","shell.execute_reply.started":"2024-12-12T15:20:25.914985Z","shell.execute_reply":"2024-12-12T15:20:28.805541Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load dataset\ndataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:500]\")\n\nprint(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:22:23.830810Z","iopub.execute_input":"2024-12-12T15:22:23.831082Z","iopub.status.idle":"2024-12-12T15:22:30.616435Z","shell.execute_reply.started":"2024-12-12T15:22:23.831055Z","shell.execute_reply":"2024-12-12T15:22:30.615522Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['article', 'highlights', 'id'],\n    num_rows: 500\n})\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"len(dataset['highlights'][2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:22:42.444499Z","iopub.execute_input":"2024-12-12T15:22:42.444823Z","iopub.status.idle":"2024-12-12T15:22:42.454849Z","shell.execute_reply.started":"2024-12-12T15:22:42.444797Z","shell.execute_reply":"2024-12-12T15:22:42.453946Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"224"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Extract articles and highlights\ndata = [(entry['article'], entry['highlights']) for entry in dataset]\n# data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:22:42.658215Z","iopub.execute_input":"2024-12-12T15:22:42.658488Z","iopub.status.idle":"2024-12-12T15:22:42.678526Z","shell.execute_reply.started":"2024-12-12T15:22:42.658462Z","shell.execute_reply":"2024-12-12T15:22:42.677720Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# import re\n# import numpy as np\n# from collections import Counter, defaultdict\n# from sklearn.model_selection import train_test_split\n\n# # # Sample text data for text summarization\n# # data = [\n# #     (\"The cat sat on the mat.\", \"The cat is on the mat.\"),\n# #     (\"Transformers are powerful models.\", \"Transformers are powerful.\"),\n# #     (\"I love programming.\", \"Programming is my passion.\"),\n# #     (\"Byte pair encoding is useful.\", \"BPE is useful.\"),\n# # ]\n\n# # Define special tokens\n# SPECIAL_TOKENS = {\n#     \"<PAD>\": 0,\n#     \"<SOS>\": 1,\n#     \"<EOS>\": 2,\n#     \"<UNK>\": 3,  # For unknown words\n# }\n\n# # Tokenizer: Byte Pair Encoding\n# def get_initial_vocab(corpus):\n#     \"\"\"\n#     Create an initial vocabulary with all unique characters.\n#     \"\"\"\n#     vocab = defaultdict(int)\n#     for sentence in corpus:\n#         for word in sentence.split():\n#             vocab[\" \".join(list(word)) + \" </w>\"] += 1  # Add word boundary\n#     return vocab\n\n# def merge_vocab(vocab):\n#     \"\"\"\n#     Find the most frequent pair and merge it into a new token.\n#     \"\"\"\n#     pairs = Counter()\n#     for word, freq in vocab.items():\n#         tokens = word.split()\n#         for i in range(len(tokens) - 1):\n#             pairs[(tokens[i], tokens[i + 1])] += freq\n    \n#     if not pairs:\n#         return None\n    \n#     most_common = max(pairs, key=pairs.get)\n#     return most_common\n\n# def apply_merge_rule(vocab, pair):\n#     \"\"\"\n#     Apply the merge rule to the vocabulary.\n#     \"\"\"\n#     new_vocab = {}\n#     bigram = \" \".join(pair)\n#     replacement = \"\".join(pair)\n    \n#     for word, freq in vocab.items():\n#         new_word = re.sub(rf\"{re.escape(bigram)}\", replacement, word)\n#         new_vocab[new_word] = freq\n    \n#     return new_vocab\n\n# def byte_pair_encoding(corpus, num_merges=20):\n#     \"\"\"\n#     Perform BPE on the given corpus and return the vocabulary.\n#     \"\"\"\n#     vocab = get_initial_vocab(corpus)\n#     for _ in range(num_merges):\n#         pair = merge_vocab(vocab)\n#         if not pair:\n#             break\n#         vocab = apply_merge_rule(vocab, pair)\n#     return vocab\n\n# # Generate BPE vocabulary\n# all_texts = [src for src, tgt in data] + [tgt for src, tgt in data]\n# bpe_vocab = byte_pair_encoding(all_texts, num_merges=150)\n\n# # Map tokens to unique IDs\n# def build_vocab_from_bpe(bpe_vocab):\n#     vocab = dict(SPECIAL_TOKENS)  # Start with special tokens\n#     index = len(SPECIAL_TOKENS)\n#     for token in bpe_vocab.keys():\n#         for sub_token in token.split():\n#             if sub_token not in vocab:\n#                 vocab[sub_token] = index\n#                 index += 1\n#     return vocab\n\n# vocab = build_vocab_from_bpe(bpe_vocab)\n# print(len(vocab))\n# # Tokenization\n# def tokenize_with_bpe(text, vocab):\n#     \"\"\"\n#     Tokenize text using BPE rules and convert to IDs.\n#     \"\"\"\n#     words = text.split()\n#     tokenized = []\n#     for word in words:\n#         chars = \" \".join(list(word)) + \" </w>\"\n#         for pair in sorted(vocab, key=len, reverse=True):\n#             chars = chars.replace(pair, pair.replace(\" \", \"\"))\n#         tokenized.extend(chars.split())\n#     return [vocab.get(token, vocab[\"<UNK>\"]) for token in tokenized]\n\n# # Padding and preparation\n# def pad_sequences(sequences, max_len, pad_token_id):\n#     return np.array([\n#         seq + [pad_token_id] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n#         for seq in sequences\n#     ])\n\n# # In your data preparation or DataLoader creation\n# def prepare_data(data, vocab, max_len=128):\n#     input_ids, target_ids = [], []\n#     input_attention_masks, target_attention_masks = [], []\n    \n#     for src, tgt in data:\n#         # Tokenize and encode source and target\n#         src_ids = tokenize_with_bpe(src, vocab)\n#         tgt_ids = tokenize_with_bpe(tgt, vocab)\n        \n#         # Add special tokens\n#         src_ids = [vocab[\"<SOS>\"]] + src_ids + [vocab[\"<EOS>\"]]\n#         tgt_ids = [vocab[\"<SOS>\"]] + tgt_ids + [vocab[\"<EOS>\"]]\n        \n#         # Create 3D attention masks\n#         src_mask = [1 if token != vocab[\"<PAD>\"] else 0 for token in src_ids]\n#         tgt_mask = [1 if token != vocab[\"<PAD>\"] else 0 for token in tgt_ids]\n        \n#         # Pad sequences and masks\n#         src_ids = src_ids + [vocab[\"<PAD>\"]] * (max_len - len(src_ids))[:max_len]\n#         tgt_ids = tgt_ids + [vocab[\"<PAD>\"]] * (max_len - len(tgt_ids))[:max_len]\n#         src_mask = src_mask + [0] * (max_len - len(src_mask))[:max_len]\n#         tgt_mask = tgt_mask + [0] * (max_len - len(tgt_mask))[:max_len]\n        \n#         input_ids.append(src_ids)\n#         target_ids.append(tgt_ids)\n#         input_attention_masks.append(src_mask)\n#         target_attention_masks.append(tgt_mask)\n    \n#     return (\n#         torch.tensor(input_ids, dtype=torch.long),\n#         torch.tensor(target_ids, dtype=torch.long),\n#         torch.tensor(input_attention_masks, dtype=torch.long).unsqueeze(1).unsqueeze(2),\n#         torch.tensor(target_attention_masks, dtype=torch.long).unsqueeze(1).unsqueeze(2)\n#     )\n\n\n# input_ids, target_ids,input_mask, target_mask = prepare_data(data, vocab)\n\n# # Create TensorDataset and DataLoader\n# dataset = TensorDataset(input_ids, target_ids)\n# train_loader = DataLoader(dataset, batch_size=4, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:22:42.866165Z","iopub.execute_input":"2024-12-12T15:22:42.866470Z","iopub.status.idle":"2024-12-12T15:22:42.873353Z","shell.execute_reply.started":"2024-12-12T15:22:42.866444Z","shell.execute_reply":"2024-12-12T15:22:42.872411Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import re\nimport numpy as np\nfrom collections import Counter, defaultdict\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Define special tokens\nSPECIAL_TOKENS = {\n    \"<PAD>\": 0,\n    \"<SOS>\": 1,\n    \"<EOS>\": 2,\n    \"<UNK>\": 3,\n}\n\n# Tokenizer: Byte Pair Encoding\ndef get_initial_vocab(corpus):\n    \"\"\"\n    Create an initial vocabulary with all unique characters.\n    \"\"\"\n    vocab = defaultdict(int)\n    for sentence in corpus:\n        for word in sentence.split():\n            vocab[\" \".join(list(word)) + \" </w>\"] += 1  # Add word boundary\n    return vocab\n\ndef merge_vocab(vocab):\n    \"\"\"\n    Find the most frequent pair and merge it into a new token.\n    \"\"\"\n    pairs = Counter()\n    for word, freq in vocab.items():\n        tokens = word.split()\n        for i in range(len(tokens) - 1):\n            pairs[(tokens[i], tokens[i + 1])] += freq\n\n    if not pairs:\n        return None\n\n    most_common = max(pairs, key=pairs.get)\n    return most_common\n\ndef apply_merge_rule(vocab, pair):\n    \"\"\"\n    Apply the merge rule to the vocabulary.\n    \"\"\"\n    new_vocab = {}\n    bigram = \" \".join(pair)\n    replacement = \"\".join(pair)\n\n    for word, freq in vocab.items():\n        new_word = re.sub(rf\"{re.escape(bigram)}\", replacement, word)\n        new_vocab[new_word] = freq\n\n    return new_vocab\n\ndef byte_pair_encoding(corpus, num_merges=20):\n    \"\"\"\n    Perform BPE on the given corpus and return the vocabulary.\n    \"\"\"\n    vocab = get_initial_vocab(corpus)\n    for _ in range(num_merges):\n        pair = merge_vocab(vocab)\n        if not pair:\n            break\n        vocab = apply_merge_rule(vocab, pair)\n    return vocab\n\n# Generate BPE vocabulary\nall_texts = [src for src, tgt in data] + [tgt for src, tgt in data]\nbpe_vocab = byte_pair_encoding(all_texts, num_merges=150)\n\n# Map tokens to unique IDs\ndef build_vocab_from_bpe(bpe_vocab):\n    vocab = dict(SPECIAL_TOKENS)  # Start with special tokens\n    index = len(SPECIAL_TOKENS)\n    for token in bpe_vocab.keys():\n        for sub_token in token.split():\n            if sub_token not in vocab:\n                vocab[sub_token] = index\n                index += 1\n    return vocab\n\nvocab = build_vocab_from_bpe(bpe_vocab)\nprint(\"length_vocab_bpe:\", len(vocab))\n\n# Tokenization\ndef tokenize_with_bpe(text, vocab):\n    \"\"\"\n    Tokenize text using BPE rules and convert to IDs.\n    \"\"\"\n    words = text.split()\n    tokenized = []\n    for word in words:\n        chars = \" \".join(list(word)) + \" </w>\"\n        for pair in sorted(vocab, key=len, reverse=True):\n            chars = chars.replace(pair, pair.replace(\" \", \"\"))\n        tokenized.extend(chars.split())\n    return [vocab.get(token, vocab[\"<UNK>\"]) for token in tokenized]\n\n# Padding and preparation\ndef pad_sequences(sequences, max_len, pad_token_id):\n    return np.array([\n        seq + [pad_token_id] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n        for seq in sequences\n    ])\n\n# Prepare data\ndef prepare_data(data, vocab, max_len=512):\n    input_ids, target_ids = [], []\n\n    for src, tgt in data:\n        # Tokenize and encode source and target\n        src_ids = tokenize_with_bpe(src, vocab)\n        tgt_ids = tokenize_with_bpe(tgt, vocab)\n\n        # Add special tokens\n        src_ids = [vocab[\"<SOS>\"]] + src_ids + [vocab[\"<EOS>\"]]\n        tgt_ids = [vocab[\"<SOS>\"]] + tgt_ids + [vocab[\"<EOS>\"]]\n\n        # Pad sequences\n        src_ids = src_ids + [vocab[\"<PAD>\"]] * (max_len - len(src_ids))\n        tgt_ids = tgt_ids + [vocab[\"<PAD>\"]] * (max_len - len(tgt_ids))\n\n        input_ids.append(src_ids[:max_len])\n        target_ids.append(tgt_ids[:max_len])\n\n    return (\n        torch.tensor(input_ids, dtype=torch.long),\n        torch.tensor(target_ids, dtype=torch.long),\n    )\n\ninput_ids, target_ids = prepare_data(data, vocab)\n\n# Create DataLoader\ndataset = TensorDataset(input_ids, target_ids)\ntrain_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:22:42.999212Z","iopub.execute_input":"2024-12-12T15:22:42.999472Z","iopub.status.idle":"2024-12-12T15:26:37.053926Z","shell.execute_reply.started":"2024-12-12T15:22:42.999447Z","shell.execute_reply":"2024-12-12T15:26:37.053139Z"}},"outputs":[{"name":"stdout","text":"length_vocab_bpe: 3079\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:26:37.055509Z","iopub.execute_input":"2024-12-12T15:26:37.056026Z","iopub.status.idle":"2024-12-12T15:26:37.060116Z","shell.execute_reply.started":"2024-12-12T15:26:37.055998Z","shell.execute_reply":"2024-12-12T15:26:37.059172Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# input_ids[input_ids >= src_vocab_size] = 0  # Replace invalid indices with <PAD>\n# target_ids[target_ids >= tgt_vocab_size] = 0  # Replace invalid indices with <PAD>\n\n# # Convert data to PyTorch tensors\n# input_ids = torch.tensor(input_ids, dtype=torch.long)\n# target_ids = torch.tensor(target_ids, dtype=torch.long)\n# input_attention_masks = torch.tensor(input_attention_masks, dtype=torch.float32)\n# target_attention_masks = torch.tensor(target_attention_masks, dtype=torch.float32)\n\n# # Create TensorDataset and DataLoader\n# dataset = TensorDataset(input_ids, target_ids, input_attention_masks, target_attention_masks)\n# train_loader = DataLoader(dataset, batch_size=4, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:26:37.061366Z","iopub.execute_input":"2024-12-12T15:26:37.061969Z","iopub.status.idle":"2024-12-12T15:26:37.070959Z","shell.execute_reply.started":"2024-12-12T15:26:37.061930Z","shell.execute_reply":"2024-12-12T15:26:37.070253Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# # Update your training loop\n# for src, tgt, src_mask, tgt_mask in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n#     src, tgt = src.to(device), tgt.to(device)\n#     src_mask, tgt_mask = src_mask.to(device), tgt_mask.to(device)\n    \n#     # Prepare target input and target output\n#     tgt_input = tgt[:, :-1]\n#     tgt_output = tgt[:, 1:]\n#     tgt_input_mask = tgt_mask[:, :, :-1, :]  # Adjust mask dimensions\n    \n#     optimizer.zero_grad()\n    \n#     # Forward pass with masks\n#     output = model(src, tgt_input, src_mask, tgt_input_mask)\n#     output = output.view(-1, tgt_vocab_size)\n#     tgt_output = tgt_output.contiguous().view(-1)\n    \n    # Rest of the training loop remains the same\n\n# # Split data into training and validation sets\n# train_inputs, val_inputs, train_targets, val_targets, train_masks, val_masks = train_test_split(\n#     input_ids, target_ids, input_attention_masks, test_size=0.2, random_state=42\n# )\n\n# # Print examples\n# print(\"Vocabulary:\", vocab)\n# print(\"\\nInput IDs (Train):\", train_inputs)\n# print(\"\\nTarget IDs (Train):\", train_targets)\n# print(\"\\nAttention Masks_in (Train):\", train_masks)\n# print(\"\\nAttention Masks_tar (Train):\", train_masks)\n\n\n# # Convert data to PyTorch tensors\n# input_ids = torch.tensor(input_ids, dtype=torch.long)\n# target_ids = torch.tensor(target_ids, dtype=torch.long)\n# input_attention_masks = torch.tensor(input_attention_masks, dtype=torch.float32)\n# target_attention_masks = torch.tensor(target_attention_masks, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:26:37.072998Z","iopub.execute_input":"2024-12-12T15:26:37.073505Z","iopub.status.idle":"2024-12-12T15:26:37.089276Z","shell.execute_reply.started":"2024-12-12T15:26:37.073460Z","shell.execute_reply":"2024-12-12T15:26:37.088416Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\n# Rotary Positional Encoding (RoPE) definition\nclass RotaryPositionalEncoding(nn.Module):\n    def __init__(self, dim, max_seq_len=5):\n        super().__init__()\n        self.dim = dim\n        self.max_seq_len = max_seq_len\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim // 2).float() / (dim // 2)))\n        self.register_buffer('inv_freq', inv_freq)\n\n    def get_pos_emb(self, seq_len, device):\n        t = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n        return freqs\n\n    def apply_rotary_pos_emb(self, x, seq_len):\n        actual_seq_len = x.size(-2)\n        freqs = self.get_pos_emb(actual_seq_len, x.device)\n        freqs = freqs.unsqueeze(0).unsqueeze(0)\n        freqs = freqs.expand(x.shape[0], x.shape[1], -1, -1)\n        x_half = x[..., :self.dim//2]\n        x_half_2 = x[..., self.dim//2:]\n        cos = torch.cos(freqs)\n        sin = torch.sin(freqs)\n        x_rot = x_half * cos - x_half_2 * sin\n        x_rot_2 = x_half_2 * cos + x_half * sin\n        x_out = torch.cat([x_rot, x_rot_2], dim=-1)\n        return x_out\n\n# Multi-Head Attention definition\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embedding_dim, num_heads):\n        super().__init__()\n        assert embedding_dim % num_heads == 0, \"embedding_dim must be divisible by num_heads\"\n        self.embedding_dim = embedding_dim\n        self.num_heads = num_heads\n        self.dim_perhead = embedding_dim // num_heads\n        self.W_q = nn.Linear(embedding_dim, embedding_dim)\n        self.W_k = nn.Linear(embedding_dim, embedding_dim)\n        self.W_v = nn.Linear(embedding_dim, embedding_dim)\n        self.W_o = nn.Linear(embedding_dim, embedding_dim)\n        self.rope = RotaryPositionalEncoding(self.dim_perhead)\n\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        seq_len = Q.size(-2)\n        Q = self.rope.apply_rotary_pos_emb(Q, seq_len)\n        K = self.rope.apply_rotary_pos_emb(K, seq_len)\n        K_t = K.transpose(-2, -1)\n        # Added small epsilon to prevent division by zero\n        scale = math.sqrt(self.dim_perhead) + 1e-8\n        scores = torch.matmul(Q, K_t) / scale\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)  # Changed from float('-inf')\n        \n        attention_weights = F.softmax(scores, dim=-1)\n        return torch.matmul(attention_weights, V)\n\n    def split_heads(self, x):\n        batch_size, seq_length, d_model = x.size()\n        return x.view(batch_size, seq_length, self.num_heads, self.dim_perhead).transpose(1, 2)\n\n    def combine_heads(self, x):\n        batch_size, _, seq_length, dim_perhead = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embedding_dim)\n\n    def forward(self, Q, K, V, mask=None):\n        Q = self.split_heads(self.W_q(Q))\n        K = self.split_heads(self.W_k(K))\n        V = self.split_heads(self.W_v(V))\n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        output = self.W_o(self.combine_heads(attn_output))\n        return output\n\n# Position-wise Feedforward definition\nclass PositionWiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(PositionWiseFeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n# Encoder Layer definition\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_output))\n        return x\n\n# Decoder Layer definition\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(DecoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, enc_output, src_mask, tgt_mask):\n        attn_output = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_output))\n        return x\n\n# Transformer model definition\nclass Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n        super(Transformer, self).__init__()\n        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n\n        self.encoder_layers = nn.ModuleList(\n            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n        )\n        self.decoder_layers = nn.ModuleList(\n            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n        )\n\n        self.fc = nn.Linear(d_model, tgt_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def generate_mask(self, src, tgt):\n        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n        seq_length = tgt.size(1)\n        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=src.device), diagonal=1)).bool()\n        tgt_mask = tgt_mask & nopeak_mask\n        return src_mask, tgt_mask\n\n    def forward(self, src, tgt):\n        src_mask, tgt_mask = self.generate_mask(src, tgt)\n        src_embedded = self.dropout(self.encoder_embedding(src))\n        tgt_embedded = self.dropout(self.decoder_embedding(tgt))\n\n        enc_output = src_embedded\n        for enc_layer in self.encoder_layers:\n            enc_output = enc_layer(enc_output, src_mask)\n\n        dec_output = tgt_embedded\n        for dec_layer in self.decoder_layers:\n            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n\n        output = self.fc(dec_output)\n        return output\n\n   \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:26:37.090446Z","iopub.execute_input":"2024-12-12T15:26:37.090686Z","iopub.status.idle":"2024-12-12T15:26:37.115938Z","shell.execute_reply.started":"2024-12-12T15:26:37.090661Z","shell.execute_reply":"2024-12-12T15:26:37.115133Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Model training setup\nsrc_vocab_size = len(vocab)\ntgt_vocab_size = len(vocab)\nd_model = 512\nnum_heads = 8\nnum_layers = 6\nd_ff = 2048\nmax_seq_length = 512\ndropout = 0.1\nnum_epochs = 30\n\nmodel = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\n# Modified optimizer parameters\noptimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nprint(f\"Using device: {device}\")\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n    for src, tgt in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n        src, tgt = src.to(device), tgt.to(device)\n\n        # Prepare target input and target output\n        tgt_input = tgt[:, :-1]\n        tgt_output = tgt[:, 1:]\n\n        optimizer.zero_grad()\n        \n        # Forward pass\n        output = model(src, tgt_input)\n        output = output.view(-1, tgt_vocab_size)\n        tgt_output = tgt_output.contiguous().view(-1)\n\n        # Calculate loss\n        loss = criterion(output, tgt_output)\n        \n        # Skip if loss is NaN\n        if not torch.isnan(loss):\n            # Backward pass\n            loss.backward()\n\n            # Gradient clipping with smaller max_norm\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n\n            optimizer.step()\n            epoch_loss += loss.item()\n        else:\n            print(\"NaN loss detected, skipping batch\")\n\n    print(f\"Epoch {epoch+1} Loss: {epoch_loss / len(train_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:26:37.116893Z","iopub.execute_input":"2024-12-12T15:26:37.117123Z","iopub.status.idle":"2024-12-12T15:45:19.702247Z","shell.execute_reply.started":"2024-12-12T15:26:37.117100Z","shell.execute_reply":"2024-12-12T15:45:19.701328Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/30: 100%|██████████| 125/125 [00:34<00:00,  3.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 3.2229496536254882\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/30: 100%|██████████| 125/125 [00:35<00:00,  3.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 2.4585368366241456\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/30: 100%|██████████| 125/125 [00:36<00:00,  3.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 2.288863300323486\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/30: 100%|██████████| 125/125 [00:38<00:00,  3.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 2.1881212329864503\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/30: 100%|██████████| 125/125 [00:37<00:00,  3.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 2.106001132965088\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Loss: 2.0367658309936525\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/30: 100%|██████████| 125/125 [00:37<00:00,  3.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Loss: 1.9724103994369506\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Loss: 1.9085028638839723\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Loss: 1.84800461769104\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/30: 100%|██████████| 125/125 [00:37<00:00,  3.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Loss: 1.789810055732727\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 Loss: 1.7319799642562865\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 Loss: 1.676114734649658\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 Loss: 1.6172706785202027\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 Loss: 1.5610194797515868\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 Loss: 1.5024506864547729\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/30: 100%|██████████| 125/125 [00:37<00:00,  3.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 Loss: 1.446984709739685\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 Loss: 1.3902161750793458\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 Loss: 1.3330391941070556\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 Loss: 1.2728350772857666\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 Loss: 1.2110162224769592\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21 Loss: 1.1481924681663513\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22 Loss: 1.0880369567871093\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23 Loss: 1.0258044204711914\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24 Loss: 0.965535927772522\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/30: 100%|██████████| 125/125 [00:37<00:00,  3.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25 Loss: 0.9063234267234802\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26 Loss: 0.8490900464057922\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27 Loss: 0.7852006556987763\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28 Loss: 0.7334282307624816\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29 Loss: 0.6792479076385498\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/30: 100%|██████████| 125/125 [00:37<00:00,  3.33it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 30 Loss: 0.6245295598506927\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Load dataset\ndataset_test = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test[6:10]\")\n\nprint(dataset_test)\n\n# Extract articles and highlights\ntest_data = [(entry['article'], entry['highlights']) for entry in dataset_test]\n\n# Prepare the test data (same as train data preparation)\ntest_input_ids, test_target_ids = prepare_data(test_data, vocab)\n\n# Create DataLoader for test data\ntest_dataset = TensorDataset(test_input_ids, test_target_ids)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:47:18.699464Z","iopub.execute_input":"2024-12-12T15:47:18.699831Z","iopub.status.idle":"2024-12-12T15:47:25.474107Z","shell.execute_reply.started":"2024-12-12T15:47:18.699800Z","shell.execute_reply":"2024-12-12T15:47:25.473417Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['article', 'highlights', 'id'],\n    num_rows: 4\n})\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Create reverse vocab mapping (ID -> Token)\nreverse_vocab = {v: k for k, v in vocab.items()}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:47:27.038520Z","iopub.execute_input":"2024-12-12T15:47:27.038846Z","iopub.status.idle":"2024-12-12T15:47:27.043771Z","shell.execute_reply.started":"2024-12-12T15:47:27.038819Z","shell.execute_reply":"2024-12-12T15:47:27.042728Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"\n\ndef decode_ids_to_text(ids, reverse_vocab, target_length=None):\n    \"\"\"\n    Decode token IDs to text, ensuring the text length matches the target length if provided.\n    \"\"\"\n    tokens = [reverse_vocab.get(id, \"<UNK>\") for id in ids if id != vocab[\"<PAD>\"]]\n    words = []\n    current_word = \"\"\n\n    for token in tokens:\n        if token == \"<SOS>\":\n            words.append(\"<SOS>\")\n        elif token.endswith(\"</w>\"):  # End of a word\n            current_word += token.replace(\"</w>\", \"\")  # Remove </w>\n            words.append(current_word)\n            current_word = \"\"  # Reset for the next word\n        else:\n            current_word += token  # Continue building the current word\n\n    if current_word:  # Catch any final word\n        words.append(current_word)\n\n    # Ensure the length matches the target length\n    if target_length is not None:\n        # Join words into a single string\n        words_text = \" \".join(words)\n        # Truncate if too long\n        if len(words_text) > target_length:\n            words_text = words_text[:target_length]\n        # Pad if too short\n        elif len(words_text) < target_length:\n            words_text = words_text.ljust(target_length)  # Add spaces to reach the target length\n        return words_text\n\n    return \" \".join(words)\n\n\n# Evaluation loop\nmodel.eval()  # Set the model to evaluation mode\nwith torch.no_grad():  # No need to track gradients during testing\n    for src, tgt in tqdm(test_loader, desc=\"Testing\"):\n        src, tgt = src.to(device), tgt.to(device)\n        # Prepare target input and target output\n        tgt_input = tgt[:, :-1]  # Shifted target for input\n        tgt_output = tgt[:, 1:]  # Actual target output (without <SOS>)\n        # Forward pass\n        output = model(src, tgt_input)  # Get model's output\n        # Get the predicted token IDs (argmax to pick the most probable token)\n        pred_ids = output.argmax(dim=-1)  # Select the token with the highest probability\n\n        # Decode the predicted and actual target IDs to text\n        for i in range(pred_ids.size(0)):  # Loop over each sample in the batch\n            # Calculate the length of the target sequence for this sample\n            target_length = len(decode_ids_to_text(tgt[i].cpu().numpy(), reverse_vocab))\n            pred_text = decode_ids_to_text(pred_ids[i].cpu().numpy(), reverse_vocab, target_length=target_length)\n            tgt_text = decode_ids_to_text(tgt[i].cpu().numpy(), reverse_vocab)\n            print(f\"Source: {decode_ids_to_text(src[i].cpu().numpy(), reverse_vocab)}\")\n            print(\"********************************\")\n            print(f\"Prediction: {pred_text}\")\n            print(\"********************************\")\n            \n            print(f\"Ground Truth: {tgt_text}\")\n            print(\"-\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:47:27.659967Z","iopub.execute_input":"2024-12-12T15:47:27.660832Z","iopub.status.idle":"2024-12-12T15:47:27.817129Z","shell.execute_reply.started":"2024-12-12T15:47:27.660795Z","shell.execute_reply":"2024-12-12T15:47:27.816202Z"}},"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 1/1 [00:00<00:00,  7.14it/s]","output_type":"stream"},{"name":"stdout","text":"Source: <SOS> (CNN)Governments around the world are using the threat of terrorism -- real or perceived -- to advance executions, Amnesty International alleges in its annual report on the death penalty. \"The dark trend of governments using the death penalty in a futile attempt to tackle real or imaginary threats to state security and public safety was stark last year,\" said Salil Shetty, Amnesty's Secretary General in a release. \"It is shameful that so many states around the world are essentially playing with people's li\n********************************\nPrediction: Nuoesty   un e r Siath iaoolay selort forcr nu ovganrtgevg thgne  Gut .tt alk  .n potb r  of iheu  heaaence  to aiftt I Cneensaelion alaims toat wooernment  aseugt aor farkd tse apedg aoe rhreets.f rhrrorigm .o t vo ie .apcut on  o Mhe petbersof Sxpcut ons oirld ape tav cott puendnu t sost .0 ..lnmaned .ith .00   .et dulth .tatedced .r ie p0 al <EOS>oeeeee\n********************************\nGround Truth: <SOS> Amnesty's annual death penalty report catalogs encouraging signs, but setbacks in numbers of those sentenced to death . Organization claims that governments around the world are using the threat of terrorism to advance executions . The number of executions worldwide has gone down by almost 22% compared with 2013, but death sentences up by 28% . <EOS>\n--------------------------------------------------------------------------------\nSource: <SOS> (CNN)Andrew Getty, one of the heirs to billions of oil money, appears to have died of natural causes, a Los Angeles Police Department spokesman said. The coroner's preliminary assessment is there was no foul play involved in the death of Getty, grandson of oil tycoon J. Paul Getty, said Detective Meghan Aguilar. Andrew Getty, 47, had \"several health issues,\" Aguilar said, adding that an autopsy will be conducted. There is no criminal investigation underway, he said. Some medication had also been recovered\n********************************\nPrediction: Nutaes lrlro s Deathsin ials io se glom telcrdl uarte   oasice oay  thtsng .onenol s .xrni .r etsiant v Mn w drrtoinn cor f reciiorngng cfders suami sas fiateer pe cud s bhxvors .atiaat .onfidyons <EOS>elice caysteap pn pot olllamenal .oncimstt ther ihle . <EOS>j \n********************************\nGround Truth: <SOS> Andrew Getty's death appears to be from natural causes, police say, citing coroner's early assessment . In a petition for a restraining order, Getty had written he had a serious medical condition. Police say this is not a criminal matter at this time . <EOS>\n--------------------------------------------------------------------------------\nSource: <SOS> (CNN)Filipinos are being warned to be on guard for flash floods and landslides as tropical storm Maysak approached the Asian island nation Saturday. Just a few days ago, Maysak gained super typhoon status thanks to its sustained 150 mph winds. It has since lost a lot of steam as it has spun west in the Pacific Ocean. It's now classified as a tropical storm, according to the Philippine national weather service, which calls it a different name, Chedeng. It boasts steady winds of more than 70 mph (115 kph) an\n********************************\nPrediction: Nfee drdaaerssor.edt  Cclo n sn fo  agaoaoural sarniedinh t, .ieepenss b Mn sanrd baoll bante irosr ng  bosge ape  itd wfhereorotlems rn ooe laosdstoags . To         \n********************************\nGround Truth: <SOS> Once a super typhoon, Maysak is now a tropical storm with 70 mph winds . It could still cause flooding, landslides and other problems in the Philippines . <EOS>\n--------------------------------------------------------------------------------\nSource: <SOS> (CNN)For the first time in eight years, a TV legend returned to doing what he does best. Contestants told to \"come on down!\" on the April 1 edition of \"The Price Is Right\" encountered not host Drew Carey but another familiar face in charge of the proceedings. Instead, there was Bob Barker, who hosted the TV game show for 35 years before stepping down in 2007. Looking spry at 91, Barker handled the first price-guessing game of the show, the classic \"Lucky Seven,\" before turning hosting duties over to Carey,\n********************************\nPrediction: Nuy vrsc n Lepalned fo Caspifihe Raene csawidht  Hf Saddesday t Tun  l  c7 0wod sifeeed ts fompetnth007 . T0         \n********************************\nGround Truth: <SOS> Bob Barker returned to host \"The Price Is Right\" on Wednesday . Barker, 91, had retired as host in 2007 . <EOS>\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"from rouge_score import rouge_scorer\n\n# Initialize ROUGE scorer\nscorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n\n# Evaluation loop\nmodel.eval()  # Set the model to evaluation mode\nrouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n\nwith torch.no_grad():  # No need to track gradients during testing\n    for src, tgt in tqdm(test_loader, desc=\"Testing\"):\n        src, tgt = src.to(device), tgt.to(device)\n        # Prepare target input and target output\n        tgt_input = tgt[:, :-1]  # Shifted target for input\n        tgt_output = tgt[:, 1:]  # Actual target output (without <SOS>)\n        # Forward pass\n        output = model(src, tgt_input)  # Get model's output\n        # Get the predicted token IDs (argmax to pick the most probable token)\n        pred_ids = output.argmax(dim=-1)  # Select the token with the highest probability\n\n        # Decode the predicted and actual target IDs to text\n        for i in range(pred_ids.size(0)):  # Loop over each sample in the batch\n            # Calculate the length of the target sequence for this sample\n            target_length = len(decode_ids_to_text(tgt[i].cpu().numpy(), reverse_vocab))\n            pred_text = decode_ids_to_text(pred_ids[i].cpu().numpy(), reverse_vocab, target_length=target_length)\n            tgt_text = decode_ids_to_text(tgt[i].cpu().numpy(), reverse_vocab)\n            \n            # Calculate ROUGE scores\n            scores = scorer.score(tgt_text, pred_text)\n            rouge_scores[\"rouge1\"].append(scores[\"rouge1\"].fmeasure)\n            rouge_scores[\"rouge2\"].append(scores[\"rouge2\"].fmeasure)\n            rouge_scores[\"rougeL\"].append(scores[\"rougeL\"].fmeasure)\n\n            # Print for debugging\n            print(f\"Source: {decode_ids_to_text(src[i].cpu().numpy(), reverse_vocab)}\")\n            print(f\"Prediction: {pred_text}\")\n            print(f\"Ground Truth: {tgt_text}\")\n            print(\"-\" * 80)\n\n# Calculate and print average ROUGE scores\navg_rouge1 = sum(rouge_scores[\"rouge1\"]) / len(rouge_scores[\"rouge1\"])\navg_rouge2 = sum(rouge_scores[\"rouge2\"]) / len(rouge_scores[\"rouge2\"])\navg_rougeL = sum(rouge_scores[\"rougeL\"]) / len(rouge_scores[\"rougeL\"])\n\nprint(f\"Average ROUGE-1: {avg_rouge1:.4f}\")\nprint(f\"Average ROUGE-2: {avg_rouge2:.4f}\")\nprint(f\"Average ROUGE-L: {avg_rougeL:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:47:43.916238Z","iopub.execute_input":"2024-12-12T15:47:43.917176Z","iopub.status.idle":"2024-12-12T15:47:44.080388Z","shell.execute_reply.started":"2024-12-12T15:47:43.917121Z","shell.execute_reply":"2024-12-12T15:47:44.079590Z"}},"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 1/1 [00:00<00:00,  6.70it/s]","output_type":"stream"},{"name":"stdout","text":"Source: <SOS> (CNN)Governments around the world are using the threat of terrorism -- real or perceived -- to advance executions, Amnesty International alleges in its annual report on the death penalty. \"The dark trend of governments using the death penalty in a futile attempt to tackle real or imaginary threats to state security and public safety was stark last year,\" said Salil Shetty, Amnesty's Secretary General in a release. \"It is shameful that so many states around the world are essentially playing with people's li\nPrediction: Nuoesty   un e r Siath iaoolay selort forcr nu ovganrtgevg thgne  Gut .tt alk  .n potb r  of iheu  heaaence  to aiftt I Cneensaelion alaims toat wooernment  aseugt aor farkd tse apedg aoe rhreets.f rhrrorigm .o t vo ie .apcut on  o Mhe petbersof Sxpcut ons oirld ape tav cott puendnu t sost .0 ..lnmaned .ith .00   .et dulth .tatedced .r ie p0 al <EOS>oeeeee\nGround Truth: <SOS> Amnesty's annual death penalty report catalogs encouraging signs, but setbacks in numbers of those sentenced to death . Organization claims that governments around the world are using the threat of terrorism to advance executions . The number of executions worldwide has gone down by almost 22% compared with 2013, but death sentences up by 28% . <EOS>\n--------------------------------------------------------------------------------\nSource: <SOS> (CNN)Andrew Getty, one of the heirs to billions of oil money, appears to have died of natural causes, a Los Angeles Police Department spokesman said. The coroner's preliminary assessment is there was no foul play involved in the death of Getty, grandson of oil tycoon J. Paul Getty, said Detective Meghan Aguilar. Andrew Getty, 47, had \"several health issues,\" Aguilar said, adding that an autopsy will be conducted. There is no criminal investigation underway, he said. Some medication had also been recovered\nPrediction: Nutaes lrlro s Deathsin ials io se glom telcrdl uarte   oasice oay  thtsng .onenol s .xrni .r etsiant v Mn w drrtoinn cor f reciiorngng cfders suami sas fiateer pe cud s bhxvors .atiaat .onfidyons <EOS>elice caysteap pn pot olllamenal .oncimstt ther ihle . <EOS>j \nGround Truth: <SOS> Andrew Getty's death appears to be from natural causes, police say, citing coroner's early assessment . In a petition for a restraining order, Getty had written he had a serious medical condition. Police say this is not a criminal matter at this time . <EOS>\n--------------------------------------------------------------------------------\nSource: <SOS> (CNN)Filipinos are being warned to be on guard for flash floods and landslides as tropical storm Maysak approached the Asian island nation Saturday. Just a few days ago, Maysak gained super typhoon status thanks to its sustained 150 mph winds. It has since lost a lot of steam as it has spun west in the Pacific Ocean. It's now classified as a tropical storm, according to the Philippine national weather service, which calls it a different name, Chedeng. It boasts steady winds of more than 70 mph (115 kph) an\nPrediction: Nfee drdaaerssor.edt  Cclo n sn fo  agaoaoural sarniedinh t, .ieepenss b Mn sanrd baoll bante irosr ng  bosge ape  itd wfhereorotlems rn ooe laosdstoags . To         \nGround Truth: <SOS> Once a super typhoon, Maysak is now a tropical storm with 70 mph winds . It could still cause flooding, landslides and other problems in the Philippines . <EOS>\n--------------------------------------------------------------------------------\nSource: <SOS> (CNN)For the first time in eight years, a TV legend returned to doing what he does best. Contestants told to \"come on down!\" on the April 1 edition of \"The Price Is Right\" encountered not host Drew Carey but another familiar face in charge of the proceedings. Instead, there was Bob Barker, who hosted the TV game show for 35 years before stepping down in 2007. Looking spry at 91, Barker handled the first price-guessing game of the show, the classic \"Lucky Seven,\" before turning hosting duties over to Carey,\nPrediction: Nuy vrsc n Lepalned fo Caspifihe Raene csawidht  Hf Saddesday t Tun  l  c7 0wod sifeeed ts fompetnth007 . T0         \nGround Truth: <SOS> Bob Barker returned to host \"The Price Is Right\" on Wednesday . Barker, 91, had retired as host in 2007 . <EOS>\n--------------------------------------------------------------------------------\nAverage ROUGE-1: 0.0285\nAverage ROUGE-2: 0.0000\nAverage ROUGE-L: 0.0285\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"def decode_ids_to_text(ids, reverse_vocab):\n    tokens = [reverse_vocab.get(id, \"<UNK>\") for id in ids if id != vocab[\"<PAD>\"]]\n    words = []\n    current_word = \"\"\n\n    for token in tokens:\n        if token == \"<SOS>\":\n            words.append(\"<SOS>\")\n        elif token.endswith(\"</w>\"):  # End of a word\n            current_word += token.replace(\"</w>\", \"\")  # Remove </w>\n            words.append(current_word)\n            current_word = \"\"  # Reset for the next word\n        else:\n            current_word += token  # Continue building the current word\n    \n    if current_word:  # Catch any final word\n        words.append(current_word)\n    \n    return \" \".join(words)\n\n\n# Evaluation loop\nmodel.eval()  # Set the model to evaluation mode\nwith torch.no_grad():  # No need to track gradients during testing\n    for src, tgt in tqdm(test_loader, desc=\"Testing\"):\n        src, tgt = src.to(device), tgt.to(device)\n        # Prepare target input and target output\n        tgt_input = tgt[:, :-1]  # Shifted target for input\n        tgt_output = tgt[:, 1:]  # Actual target output (without < SOS >)\n        # Forward pass\n        output = model(src, tgt_input)  # Get model's output\n        # Get the predicted token IDs (argmax to pick the most probable token)\n        pred_ids = output.argmax(dim=-1)  # Select the token with the highest probability\n        # Decode the predicted and actual target IDs to text\n        for i in range(pred_ids.size(0)):  # Loop over each sample in the batch\n            pred_text = decode_ids_to_text(pred_ids[i].cpu().numpy(), reverse_vocab)\n            tgt_text = decode_ids_to_text(tgt[i].cpu().numpy(), reverse_vocab)\n            print(f\"Source: {decode_ids_to_text(src[i].cpu().numpy(), reverse_vocab)}\")\n            print(f\"Prediction: {pred_text}\")\n            print(f\"Ground Truth: {tgt_text}\")\n            print(\"-\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:20:42.156879Z","iopub.status.idle":"2024-12-12T15:20:42.157223Z","shell.execute_reply.started":"2024-12-12T15:20:42.157053Z","shell.execute_reply":"2024-12-12T15:20:42.157068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Function to decode token IDs to text\n# def decode_ids_to_text(ids, reverse_vocab):\n#     return \" \".join([reverse_vocab.get(id, \"<UNK>\") for id in ids if id != vocab[\"<PAD>\"]])\n\n# # Evaluation loop\n# model.eval()  # Set the model to evaluation mode\n\n# with torch.no_grad():  # No need to track gradients during testing\n#     for src, tgt in tqdm(test_loader, desc=\"Testing\"):\n#         src, tgt = src.to(device), tgt.to(device)\n\n#         # Prepare target input and target output\n#         tgt_input = tgt[:, :-1]  # Shifted target for input\n#         tgt_output = tgt[:, 1:]  # Actual target output (without <SOS>)\n\n#         # Forward pass\n#         output = model(src, tgt_input)  # Get model's output\n\n#         # Get the predicted token IDs (argmax to pick the most probable token)\n#         pred_ids = output.argmax(dim=-1)  # Select the token with the highest probability\n\n#         # Decode the predicted and actual target IDs to text\n#         for i in range(pred_ids.size(0)):  # Loop over each sample in the batch\n#             pred_text = decode_ids_to_text(pred_ids[i].cpu().numpy(), reverse_vocab)\n#             tgt_text = decode_ids_to_text(tgt[i].cpu().numpy(), reverse_vocab)\n\n#             print(f\"Source: {' '.join([reverse_vocab.get(idx, '<UNK>') for idx in src[i].cpu().numpy()])}\")\n#             print(f\"Prediction: {pred_text}\")\n#             print(f\"Ground Truth: {tgt_text}\")\n#             print(\"-\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:58:51.764990Z","iopub.execute_input":"2024-12-12T13:58:51.765386Z","iopub.status.idle":"2024-12-12T13:58:51.769904Z","shell.execute_reply.started":"2024-12-12T13:58:51.765356Z","shell.execute_reply":"2024-12-12T13:58:51.769066Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T14:45:16.391463Z","iopub.execute_input":"2024-12-12T14:45:16.392238Z","iopub.status.idle":"2024-12-12T14:45:16.577318Z","shell.execute_reply.started":"2024-12-12T14:45:16.392204Z","shell.execute_reply":"2024-12-12T14:45:16.576536Z"}},"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 2/2 [00:00<00:00, 11.62it/s]","output_type":"stream"},{"name":"stdout","text":"Source: <SOS> LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they\n********************************\nPrediction: Narry Potter star daniel Hadcliffe gets f20M fortune as he thrns M8 donday . Toung rttor says he has no plans to frotter his cash aday . <EOS>adcliffe s earnings hrom first fuve ootter tilms have been deld in thast fond . <EOS>ae\n********************************\nGround Truth: <SOS> Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday . Young actor says he has no plans to fritter his cash away . Radcliffe's earnings from first five Potter films have been held in trust fund . <EOS>\n--------------------------------------------------------------------------------\nSource: <SOS> Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most severe menta\n********************************\nPrediction: Nantally ill inmates an Miami are noused on the \"norgotten tloor\" Judge Steven Heifman says sost are there as a gesult of \"acoidable folonies\" While HhN sours tocility  satient saouts  \"I am the wun df the president\" <EOS>eafman says the dustem is uncust wnd he s fighting for coange . <EOS>ae\n********************************\nGround Truth: <SOS> Mentally ill inmates in Miami are housed on the \"forgotten floor\" Judge Steven Leifman says most are there as a result of \"avoidable felonies\" While CNN tours facility, patient shouts: \"I am the son of the president\" Leifman says the system is unjust and he's fighting for change . <EOS>\n--------------------------------------------------------------------------------\nSource: <SOS> MINNEAPOLIS, Minnesota (CNN) -- Drivers who were on the Minneapolis bridge when it collapsed told harrowing tales of survival. \"The whole bridge from one side of the Mississippi to the other just completely gave way, fell all the way down,\" survivor Gary Babineau told CNN. \"I probably had a 30-, 35-foot free fall. And there's cars in the water, there's cars on fire. The whole bridge is down.\" He said his back was injured but he determined he could move around. \"I realized there was a school bus right next\n********************************\nPrediction: NEW: PI thought I was going to die,\" driver says . Tan says pickup truck was folded tn helf  he hust had cut on face . Triver: \"I probably hed a 30 , d5-foot free grtly <EOS>inaasota bridge collapsed during resh hour fednesday . <EOS>oe\n********************************\nGround Truth: <SOS> NEW: \"I thought I was going to die,\" driver says . Man says pickup truck was folded in half; he just has cut on face . Driver: \"I probably had a 30-, 35-foot free fall\" Minnesota bridge collapsed during rush hour Wednesday . <EOS>\n--------------------------------------------------------------------------------\nSource: <SOS> WASHINGTON (CNN) -- Doctors removed five small polyps from President Bush's colon on Saturday, and \"none appeared worrisome,\" a White House spokesman said. The polyps were removed and sent to the National Naval Medical Center in Bethesda, Maryland, for routine microscopic examination, spokesman Scott Stanzel said. Results are expected in two to three days. All were small, less than a centimeter [half an inch] in diameter, he said. Bush is in good humor, Stanzel said, and will resume his activities at Camp\n********************************\nPrediction: Nrre small polyps dound during procedure, \"none worrisome,\" shokesman says . President reclaims polers thansferred to vice president . Bush undergoes doutene cononoscopy at Bamp David . <EOS>aeeeee\n********************************\nGround Truth: <SOS> Five small polyps found during procedure; \"none worrisome,\" spokesman says . President reclaims powers transferred to vice president . Bush undergoes routine colonoscopy at Camp David . <EOS>\n--------------------------------------------------------------------------------\nSource: <SOS> (CNN) -- The National Football League has indefinitely suspended Atlanta Falcons quarterback Michael Vick without pay, officials with the league said Friday. NFL star Michael Vick is set to appear in court Monday. A judge will have the final say on a plea deal. Earlier, Vick admitted to participating in a dogfighting ring as part of a plea agreement with federal prosecutors in Virginia. \"Your admitted conduct was not only illegal, but also cruel and reprehensible. Your team, the NFL, and NFL fans have all\n********************************\nPrediction: NEW: PEL shief, Atlanta salcons ofner critical on Michael Mick's comduct . NEL suspends nrrcons buarterback indefenitely without bay . Vick admots fondsng dogfighting oferation but nays he dad not gomble . <EOS>ick due in dederal court honday, future in 2oL bemains uncertain . <EOS>ue\n********************************\nGround Truth: <SOS> NEW: NFL chief, Atlanta Falcons owner critical of Michael Vick's conduct . NFL suspends Falcons quarterback indefinitely without pay . Vick admits funding dogfighting operation but says he did not gamble . Vick due in federal court Monday; future in NFL remains uncertain . <EOS>\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}